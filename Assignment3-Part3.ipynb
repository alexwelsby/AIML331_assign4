{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4aa71143-c22d-40fb-bb84-9b48366cc619",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import torchvision.transforms as transforms\n",
    "from dataset_wrapper import get_pet_datasets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5639ccce-8ba9-4a7e-b3ad-eb5145437459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: None\n",
      "torch.Size([4, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "img_h = 128;\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_pet_datasets(img_width=img_h, img_height=img_h,root_path='./data' )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "#selecting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#printing because my main kernel wants to be stuck on CPU-only pytorch fsr\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "# Mini-batch images and labels.\n",
    "images_onebatch, labels = next(data_iter)\n",
    "print(images_onebatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "119f86a0-720c-48c1-b306-16a766569bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim), #expansion to four folder\n",
    "            nn.GELU(), # Gaussian Error Linear Units (GELUs)\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim), #reduce the dimensionality back\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4421235e-7ac3-496f-98e1-64bc2a640e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module): # This class inherits from nn.Module and defines the architecture of the neural network.\n",
    "    #In PyTorch, nn.Module is the base class for all neural network modules. It provides a set of functionalities that are \n",
    "    # commonly required when building neural networks, such as automatic gradient computation, model saving/loading, and simple layer management.\n",
    "    def __init__(self, num_classes=4):\n",
    "        # In PyTorch, custom neural networks are created by subclassing the nn.Module class and defining layers inside its __init__() method. \n",
    "        #The layers you define as class members (e.g., self.conv1, self.fc1) are automatically registered and tracked by PyTorch.\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        #super() is used to call the __init__ method of the parent class nn.Module, without super(), you would lose the setup required for PyTorch\n",
    "        #to properly manage the layers, parameters, and gradients in your neural network model.\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(7*7*32, num_classes)\n",
    "        self.sqex  = squeezeAndExcite(n_features)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #The forward() method defines how the input data flows through the network (i.e., the computation graph). \n",
    "        #It's where you define the sequence of operations (e.g., layers, activations) the input goes through to produce the output.\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.sqex(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out) because we want to use this in squeeze and excitation\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b76085c9-a58e-484b-9b82-7945ea545093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "def generate_charts(num_layers, activation, batch_norm, residuals, train_losses, val_accuracies):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss Curve \\n({num_layers} layers, {activation}, batchnorm={batch_norm}, residuals={residuals})')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label=f'Validation Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title(f'Validation Accuracy Curve \\n({num_layers} layers, {activation}, batchnorm={batch_norm}, residuals={residuals})')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'layers{num_layers}_batch{batch_norm}_residuals{residuals}_{activation}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d71cc717-e710-4b00-b6a7-3e3304f7e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, descript, batch_size, device):\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "    # Disable gradient calculation for efficiency\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "            \n",
    "        new_acc_descript = '       Test Accuracy of the model on the 10000 test images: {:.2f} %'.format(100 * correct / total)\n",
    "        append_text_to_file('AllAccuracies.txt', descript + \"\\n\" + new_acc_descript)\n",
    "        print(descript + \"\\n\" + new_acc_descript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0cd45ad-f063-481a-a4fc-1df2f1862faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_text_to_file(file_path, text_to_append):\n",
    "    try:\n",
    "        with open(file_path, 'a') as file:\n",
    "            file.write(text_to_append + '\\n')\n",
    "        print(f\"Text appended to {file_path} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba4cc49d-fa9a-48d8-a828-68ac2a61e776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_models_from_file(filepath):\n",
    "    models_ive_trained = []\n",
    "\n",
    "    # Read file\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Define a regex pattern to capture required parameters\n",
    "    pattern = re.compile(\n",
    "        r\"pos_embedding: (\\w+); num_heads: (\\d+); num_layers:(\\d+); patch_size: (\\d+)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Process every line to match pattern\n",
    "    for line in lines:\n",
    "        match = pattern.search(line)\n",
    "        if match:\n",
    "            pos_embedding, num_heads, num_layers, patch_size = match.groups()\n",
    "            model_string = f\"{pos_embedding}{num_heads}{num_layers}{patch_size}\"\n",
    "            models_ive_trained.append(model_string)\n",
    "\n",
    "    return models_ive_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "736e4f96-e890-4e90-9d1f-b0e27937ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, descript, batch_size, device):\n",
    "    # --- Test the model ---\n",
    "    model.eval()  # evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)# choose the class that have the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "        #Remember guessing randomly among 10 classes would be about 25% accuracy\n",
    "    \n",
    "    new_acc_descript = f'       Test Accuracy of the model on the {total} test images: {(100 * correct / total)}'\n",
    "    append_text_to_file('Part2AllAccuracies.txt', descript + \"\\n\" + new_acc_descript)\n",
    "    print(descript + \"\\n\" + new_acc_descript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3715ff23-7f44-4bf1-ba80-9d14d54dcc36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model):\n",
    "    model.eval()\n",
    "    correct = 0                       \n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy\n",
    "    #print(f'Validation Accuracy after epoch {epoch+1}: {accuracy:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0269fe16-4387-444b-9822-d7e0345388d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class squeezeAndExcite(nn.Module):\n",
    "    #The block has a convolutional block as an input... called by ConvNet with 16 features\n",
    "    def __init__(self, n_features, reduction=16):\n",
    "        print(ConvNet.shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.avg_pool2d(x, kernel_size=x.size()[2:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "39e56870-c476-49f7-a49a-d001fdb0e177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 32, 32, 32])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not squeezeAndExcite",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 50\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[38;5;66;03m#now we do validation. exit training mode\u001b[39;00m\n\u001b[0;32m     46\u001b[0m         val_accuracies\u001b[38;5;241m.\u001b[39mappend(validate_model(model))\n\u001b[1;32m---> 50\u001b[0m run_model()\n",
      "Cell \u001b[1;32mIn[28], line 32\u001b[0m, in \u001b[0;36mrun_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[0;32m     31\u001b[0m newOut \u001b[38;5;241m=\u001b[39m squeezeAndExcite(outputs)\n\u001b[1;32m---> 32\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(newOut, labels)\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Backward and optimize\u001b[39;00m\n\u001b[0;32m     35\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1185\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m-> 1185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mcross_entropy(\u001b[38;5;28minput\u001b[39m, target, weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight,\n\u001b[0;32m   1186\u001b[0m                            ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index, reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduction,\n\u001b[0;32m   1187\u001b[0m                            label_smoothing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabel_smoothing)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:3086\u001b[0m, in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3084\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   3085\u001b[0m     reduction \u001b[38;5;241m=\u001b[39m _Reduction\u001b[38;5;241m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3086\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_nn\u001b[38;5;241m.\u001b[39mcross_entropy_loss(\u001b[38;5;28minput\u001b[39m, target, weight, _Reduction\u001b[38;5;241m.\u001b[39mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[1;31mTypeError\u001b[0m: cross_entropy_loss(): argument 'input' (position 1) must be Tensor, not squeezeAndExcite"
     ]
    }
   ],
   "source": [
    "def run_model():\n",
    "    #hyperparameters\n",
    "    num_epochs = 25\n",
    "    num_classes = 4\n",
    "    batch_size = 100\n",
    "    learning_rate = 0.001\n",
    "\n",
    "    #selecting device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    #printing because my main kernel wants to be stuck on CPU-only pytorch fsr\n",
    "\n",
    "    model = ConvNet(num_classes=num_classes).to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    train_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Train the model\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        for i, (images, labels) in enumerate(train_loader):\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(newOut, labels)\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()                            \n",
    "            if (i+1) % 50 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "        \n",
    "        avg_loss = epoch_loss / total_step\n",
    "        train_losses.append(avg_loss)\n",
    "\n",
    "        #now we do validation. exit training mode\n",
    "        val_accuracies.append(validate_model(model))\n",
    "        \n",
    "        \n",
    "\n",
    "run_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd77f1d-6e85-43e7-82bd-caaefa90cc36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
