{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6190b34c-7a6c-4a73-b021-864440820a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision.transforms as transforms\n",
    "from dataset_wrapper import get_pet_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23288ce5-aa03-428d-a50f-fa569283a76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolutional neural network (two convolutional layers)\n",
    "class ConvNet(nn.Module): # This class inherits from nn.Module and defines the architecture of the neural network.\n",
    "    #In PyTorch, nn.Module is the base class for all neural network modules. It provides a set of functionalities that are \n",
    "    # commonly required when building neural networks, such as automatic gradient computation, model saving/loading, and simple layer management.\n",
    "    def __init__(self, num_classes=4):\n",
    "        # In PyTorch, custom neural networks are created by subclassing the nn.Module class and defining layers inside its __init__() method. \n",
    "        #The layers you define as class members (e.g., self.conv1, self.fc1) are automatically registered and tracked by PyTorch.\n",
    "        super(ConvNet, self).__init__()\n",
    "\n",
    "        #super() is used to call the __init__ method of the parent class nn.Module, without super(), you would lose the setup required for PyTorch\n",
    "        #to properly manage the layers, parameters, and gradients in your neural network model.\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.fc = nn.Linear(32*32*32, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        #The forward() method defines how the input data flows through the network (i.e., the computation graph). \n",
    "        #It's where you define the sequence of operations (e.g., layers, activations) the input goes through to produce the output.\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f1840560-caff-4a34-adae-56effabef6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [1/58], Loss: 2.2918\n",
      "Epoch [1/5], Step [2/58], Loss: 2.9855\n",
      "Epoch [1/5], Step [3/58], Loss: 11.4101\n",
      "Epoch [1/5], Step [4/58], Loss: 12.8035\n",
      "Epoch [1/5], Step [5/58], Loss: 13.4466\n",
      "Epoch [1/5], Step [6/58], Loss: 11.1134\n",
      "Epoch [1/5], Step [7/58], Loss: 10.7866\n",
      "Epoch [1/5], Step [8/58], Loss: 9.7071\n",
      "Epoch [1/5], Step [9/58], Loss: 2.9642\n",
      "Epoch [1/5], Step [10/58], Loss: 6.0615\n",
      "Epoch [1/5], Step [11/58], Loss: 5.1869\n",
      "Epoch [1/5], Step [12/58], Loss: 6.1744\n",
      "Epoch [1/5], Step [13/58], Loss: 4.9648\n",
      "Epoch [1/5], Step [14/58], Loss: 3.7653\n",
      "Epoch [1/5], Step [15/58], Loss: 2.5995\n",
      "Epoch [1/5], Step [16/58], Loss: 4.8229\n",
      "Epoch [1/5], Step [17/58], Loss: 4.7022\n",
      "Epoch [1/5], Step [18/58], Loss: 2.7527\n",
      "Epoch [1/5], Step [19/58], Loss: 3.2517\n",
      "Epoch [1/5], Step [20/58], Loss: 3.5825\n",
      "Epoch [1/5], Step [21/58], Loss: 3.4756\n",
      "Epoch [1/5], Step [22/58], Loss: 2.4251\n",
      "Epoch [1/5], Step [23/58], Loss: 1.7666\n",
      "Epoch [1/5], Step [24/58], Loss: 3.0760\n",
      "Epoch [1/5], Step [25/58], Loss: 2.9883\n",
      "Epoch [1/5], Step [26/58], Loss: 2.3869\n",
      "Epoch [1/5], Step [27/58], Loss: 1.7682\n",
      "Epoch [1/5], Step [28/58], Loss: 2.0989\n",
      "Epoch [1/5], Step [29/58], Loss: 2.1836\n",
      "Epoch [1/5], Step [30/58], Loss: 1.8723\n",
      "Epoch [1/5], Step [31/58], Loss: 2.1682\n",
      "Epoch [1/5], Step [32/58], Loss: 2.4931\n",
      "Epoch [1/5], Step [33/58], Loss: 1.8127\n",
      "Epoch [1/5], Step [34/58], Loss: 1.7490\n",
      "Epoch [1/5], Step [35/58], Loss: 1.7982\n",
      "Epoch [1/5], Step [36/58], Loss: 1.7819\n",
      "Epoch [1/5], Step [37/58], Loss: 1.5415\n",
      "Epoch [1/5], Step [38/58], Loss: 1.3871\n",
      "Epoch [1/5], Step [39/58], Loss: 1.6313\n",
      "Epoch [1/5], Step [40/58], Loss: 1.5246\n",
      "Epoch [1/5], Step [41/58], Loss: 1.4973\n",
      "Epoch [1/5], Step [42/58], Loss: 1.6905\n",
      "Epoch [1/5], Step [43/58], Loss: 1.6502\n",
      "Epoch [1/5], Step [44/58], Loss: 1.3149\n",
      "Epoch [1/5], Step [45/58], Loss: 1.3913\n",
      "Epoch [1/5], Step [46/58], Loss: 1.6781\n",
      "Epoch [1/5], Step [47/58], Loss: 1.3430\n",
      "Epoch [1/5], Step [48/58], Loss: 1.4853\n",
      "Epoch [1/5], Step [49/58], Loss: 1.3570\n",
      "Epoch [1/5], Step [50/58], Loss: 1.4112\n",
      "Epoch [1/5], Step [51/58], Loss: 1.4129\n",
      "Epoch [1/5], Step [52/58], Loss: 1.4647\n",
      "Epoch [1/5], Step [53/58], Loss: 1.2937\n",
      "Epoch [1/5], Step [54/58], Loss: 1.5238\n",
      "Epoch [1/5], Step [55/58], Loss: 1.5326\n",
      "Epoch [1/5], Step [56/58], Loss: 1.5442\n",
      "Epoch [1/5], Step [57/58], Loss: 1.7461\n",
      "Epoch [1/5], Step [58/58], Loss: 1.3741\n",
      "Epoch [2/5], Step [1/58], Loss: 1.3206\n",
      "Epoch [2/5], Step [2/58], Loss: 1.6246\n",
      "Epoch [2/5], Step [3/58], Loss: 1.2713\n",
      "Epoch [2/5], Step [4/58], Loss: 1.4527\n",
      "Epoch [2/5], Step [5/58], Loss: 1.5221\n",
      "Epoch [2/5], Step [6/58], Loss: 1.4021\n",
      "Epoch [2/5], Step [7/58], Loss: 1.6948\n",
      "Epoch [2/5], Step [8/58], Loss: 1.4388\n",
      "Epoch [2/5], Step [9/58], Loss: 1.4108\n",
      "Epoch [2/5], Step [10/58], Loss: 1.6281\n",
      "Epoch [2/5], Step [11/58], Loss: 1.1957\n",
      "Epoch [2/5], Step [12/58], Loss: 1.6847\n",
      "Epoch [2/5], Step [13/58], Loss: 2.1343\n",
      "Epoch [2/5], Step [14/58], Loss: 1.2957\n",
      "Epoch [2/5], Step [15/58], Loss: 1.4341\n",
      "Epoch [2/5], Step [16/58], Loss: 1.6511\n",
      "Epoch [2/5], Step [17/58], Loss: 1.2637\n",
      "Epoch [2/5], Step [18/58], Loss: 1.4517\n",
      "Epoch [2/5], Step [19/58], Loss: 1.2603\n",
      "Epoch [2/5], Step [20/58], Loss: 1.4486\n",
      "Epoch [2/5], Step [21/58], Loss: 1.4129\n",
      "Epoch [2/5], Step [22/58], Loss: 1.3657\n",
      "Epoch [2/5], Step [23/58], Loss: 1.4178\n",
      "Epoch [2/5], Step [24/58], Loss: 1.3981\n",
      "Epoch [2/5], Step [25/58], Loss: 1.6078\n",
      "Epoch [2/5], Step [26/58], Loss: 1.3579\n",
      "Epoch [2/5], Step [27/58], Loss: 1.2399\n",
      "Epoch [2/5], Step [28/58], Loss: 1.2981\n",
      "Epoch [2/5], Step [29/58], Loss: 1.2412\n",
      "Epoch [2/5], Step [30/58], Loss: 1.3413\n",
      "Epoch [2/5], Step [31/58], Loss: 1.2976\n",
      "Epoch [2/5], Step [32/58], Loss: 1.3265\n",
      "Epoch [2/5], Step [33/58], Loss: 1.3456\n",
      "Epoch [2/5], Step [34/58], Loss: 1.3543\n",
      "Epoch [2/5], Step [35/58], Loss: 1.2605\n",
      "Epoch [2/5], Step [36/58], Loss: 1.3901\n",
      "Epoch [2/5], Step [37/58], Loss: 1.1453\n",
      "Epoch [2/5], Step [38/58], Loss: 1.2975\n",
      "Epoch [2/5], Step [39/58], Loss: 1.1528\n",
      "Epoch [2/5], Step [40/58], Loss: 1.4346\n",
      "Epoch [2/5], Step [41/58], Loss: 1.4956\n",
      "Epoch [2/5], Step [42/58], Loss: 1.2866\n",
      "Epoch [2/5], Step [43/58], Loss: 1.2900\n",
      "Epoch [2/5], Step [44/58], Loss: 1.3109\n",
      "Epoch [2/5], Step [45/58], Loss: 1.3394\n",
      "Epoch [2/5], Step [46/58], Loss: 1.3101\n",
      "Epoch [2/5], Step [47/58], Loss: 1.2667\n",
      "Epoch [2/5], Step [48/58], Loss: 1.3872\n",
      "Epoch [2/5], Step [49/58], Loss: 1.2366\n",
      "Epoch [2/5], Step [50/58], Loss: 1.2576\n",
      "Epoch [2/5], Step [51/58], Loss: 1.1487\n",
      "Epoch [2/5], Step [52/58], Loss: 1.3758\n",
      "Epoch [2/5], Step [53/58], Loss: 1.3034\n",
      "Epoch [2/5], Step [54/58], Loss: 1.3175\n",
      "Epoch [2/5], Step [55/58], Loss: 1.1472\n",
      "Epoch [2/5], Step [56/58], Loss: 1.3247\n",
      "Epoch [2/5], Step [57/58], Loss: 1.2603\n",
      "Epoch [2/5], Step [58/58], Loss: 1.0659\n",
      "Epoch [3/5], Step [1/58], Loss: 1.1517\n",
      "Epoch [3/5], Step [2/58], Loss: 1.4566\n",
      "Epoch [3/5], Step [3/58], Loss: 1.2972\n",
      "Epoch [3/5], Step [4/58], Loss: 1.4678\n",
      "Epoch [3/5], Step [5/58], Loss: 1.0716\n",
      "Epoch [3/5], Step [6/58], Loss: 1.1932\n",
      "Epoch [3/5], Step [7/58], Loss: 1.3283\n",
      "Epoch [3/5], Step [8/58], Loss: 1.2991\n",
      "Epoch [3/5], Step [9/58], Loss: 1.3212\n",
      "Epoch [3/5], Step [10/58], Loss: 1.3175\n",
      "Epoch [3/5], Step [11/58], Loss: 1.1682\n",
      "Epoch [3/5], Step [12/58], Loss: 1.2173\n",
      "Epoch [3/5], Step [13/58], Loss: 1.3116\n",
      "Epoch [3/5], Step [14/58], Loss: 1.2161\n",
      "Epoch [3/5], Step [15/58], Loss: 1.1414\n",
      "Epoch [3/5], Step [16/58], Loss: 1.1989\n",
      "Epoch [3/5], Step [17/58], Loss: 1.1033\n",
      "Epoch [3/5], Step [18/58], Loss: 1.6483\n",
      "Epoch [3/5], Step [19/58], Loss: 1.3108\n",
      "Epoch [3/5], Step [20/58], Loss: 1.3640\n",
      "Epoch [3/5], Step [21/58], Loss: 1.8278\n",
      "Epoch [3/5], Step [22/58], Loss: 1.1033\n",
      "Epoch [3/5], Step [23/58], Loss: 1.8568\n",
      "Epoch [3/5], Step [24/58], Loss: 1.9765\n",
      "Epoch [3/5], Step [25/58], Loss: 1.7390\n",
      "Epoch [3/5], Step [26/58], Loss: 2.0183\n",
      "Epoch [3/5], Step [27/58], Loss: 1.5982\n",
      "Epoch [3/5], Step [28/58], Loss: 1.8851\n",
      "Epoch [3/5], Step [29/58], Loss: 1.4883\n",
      "Epoch [3/5], Step [30/58], Loss: 1.5860\n",
      "Epoch [3/5], Step [31/58], Loss: 2.0218\n",
      "Epoch [3/5], Step [32/58], Loss: 1.4046\n",
      "Epoch [3/5], Step [33/58], Loss: 1.6833\n",
      "Epoch [3/5], Step [34/58], Loss: 1.8283\n",
      "Epoch [3/5], Step [35/58], Loss: 1.7186\n",
      "Epoch [3/5], Step [36/58], Loss: 1.5215\n",
      "Epoch [3/5], Step [37/58], Loss: 1.8068\n",
      "Epoch [3/5], Step [38/58], Loss: 1.7047\n",
      "Epoch [3/5], Step [39/58], Loss: 1.4801\n",
      "Epoch [3/5], Step [40/58], Loss: 1.6739\n",
      "Epoch [3/5], Step [41/58], Loss: 1.2844\n",
      "Epoch [3/5], Step [42/58], Loss: 1.9541\n",
      "Epoch [3/5], Step [43/58], Loss: 1.3361\n",
      "Epoch [3/5], Step [44/58], Loss: 1.3653\n",
      "Epoch [3/5], Step [45/58], Loss: 1.7885\n",
      "Epoch [3/5], Step [46/58], Loss: 1.5169\n",
      "Epoch [3/5], Step [47/58], Loss: 1.5808\n",
      "Epoch [3/5], Step [48/58], Loss: 1.4531\n",
      "Epoch [3/5], Step [49/58], Loss: 1.5652\n",
      "Epoch [3/5], Step [50/58], Loss: 1.0340\n",
      "Epoch [3/5], Step [51/58], Loss: 1.4289\n",
      "Epoch [3/5], Step [52/58], Loss: 1.2008\n",
      "Epoch [3/5], Step [53/58], Loss: 1.1260\n",
      "Epoch [3/5], Step [54/58], Loss: 1.2471\n",
      "Epoch [3/5], Step [55/58], Loss: 1.0262\n",
      "Epoch [3/5], Step [56/58], Loss: 1.2779\n",
      "Epoch [3/5], Step [57/58], Loss: 1.0538\n",
      "Epoch [3/5], Step [58/58], Loss: 1.0796\n",
      "Epoch [4/5], Step [1/58], Loss: 1.2818\n",
      "Epoch [4/5], Step [2/58], Loss: 1.1947\n",
      "Epoch [4/5], Step [3/58], Loss: 1.8965\n",
      "Epoch [4/5], Step [4/58], Loss: 1.0581\n",
      "Epoch [4/5], Step [5/58], Loss: 1.2283\n",
      "Epoch [4/5], Step [6/58], Loss: 1.1826\n",
      "Epoch [4/5], Step [7/58], Loss: 1.1244\n",
      "Epoch [4/5], Step [8/58], Loss: 1.3350\n",
      "Epoch [4/5], Step [9/58], Loss: 1.1185\n",
      "Epoch [4/5], Step [10/58], Loss: 1.2599\n",
      "Epoch [4/5], Step [11/58], Loss: 1.1254\n",
      "Epoch [4/5], Step [12/58], Loss: 1.2260\n",
      "Epoch [4/5], Step [13/58], Loss: 1.4060\n",
      "Epoch [4/5], Step [14/58], Loss: 1.3938\n",
      "Epoch [4/5], Step [15/58], Loss: 1.4509\n",
      "Epoch [4/5], Step [16/58], Loss: 1.2259\n",
      "Epoch [4/5], Step [17/58], Loss: 1.4181\n",
      "Epoch [4/5], Step [18/58], Loss: 1.2748\n",
      "Epoch [4/5], Step [19/58], Loss: 1.3189\n",
      "Epoch [4/5], Step [20/58], Loss: 1.1804\n",
      "Epoch [4/5], Step [21/58], Loss: 1.4816\n",
      "Epoch [4/5], Step [22/58], Loss: 1.1435\n",
      "Epoch [4/5], Step [23/58], Loss: 1.2309\n",
      "Epoch [4/5], Step [24/58], Loss: 1.3420\n",
      "Epoch [4/5], Step [25/58], Loss: 1.4607\n",
      "Epoch [4/5], Step [26/58], Loss: 1.3500\n",
      "Epoch [4/5], Step [27/58], Loss: 1.3278\n",
      "Epoch [4/5], Step [28/58], Loss: 1.0406\n",
      "Epoch [4/5], Step [29/58], Loss: 1.2137\n",
      "Epoch [4/5], Step [30/58], Loss: 1.2045\n",
      "Epoch [4/5], Step [31/58], Loss: 1.0170\n",
      "Epoch [4/5], Step [32/58], Loss: 1.1926\n",
      "Epoch [4/5], Step [33/58], Loss: 1.1678\n",
      "Epoch [4/5], Step [34/58], Loss: 1.1279\n",
      "Epoch [4/5], Step [35/58], Loss: 1.3422\n",
      "Epoch [4/5], Step [36/58], Loss: 1.0996\n",
      "Epoch [4/5], Step [37/58], Loss: 1.1381\n",
      "Epoch [4/5], Step [38/58], Loss: 1.3717\n",
      "Epoch [4/5], Step [39/58], Loss: 1.3624\n",
      "Epoch [4/5], Step [40/58], Loss: 1.1550\n",
      "Epoch [4/5], Step [41/58], Loss: 1.0252\n",
      "Epoch [4/5], Step [42/58], Loss: 1.1748\n",
      "Epoch [4/5], Step [43/58], Loss: 1.0524\n",
      "Epoch [4/5], Step [44/58], Loss: 1.3049\n",
      "Epoch [4/5], Step [45/58], Loss: 1.2791\n",
      "Epoch [4/5], Step [46/58], Loss: 1.1602\n",
      "Epoch [4/5], Step [47/58], Loss: 1.2473\n",
      "Epoch [4/5], Step [48/58], Loss: 1.1677\n",
      "Epoch [4/5], Step [49/58], Loss: 1.1042\n",
      "Epoch [4/5], Step [50/58], Loss: 1.2417\n",
      "Epoch [4/5], Step [51/58], Loss: 1.2347\n",
      "Epoch [4/5], Step [52/58], Loss: 1.2621\n",
      "Epoch [4/5], Step [53/58], Loss: 1.0388\n",
      "Epoch [4/5], Step [54/58], Loss: 1.2335\n",
      "Epoch [4/5], Step [55/58], Loss: 1.1791\n",
      "Epoch [4/5], Step [56/58], Loss: 1.0163\n",
      "Epoch [4/5], Step [57/58], Loss: 1.3168\n",
      "Epoch [4/5], Step [58/58], Loss: 0.8507\n",
      "Epoch [5/5], Step [1/58], Loss: 1.0894\n",
      "Epoch [5/5], Step [2/58], Loss: 1.0428\n",
      "Epoch [5/5], Step [3/58], Loss: 1.5275\n",
      "Epoch [5/5], Step [4/58], Loss: 1.0458\n",
      "Epoch [5/5], Step [5/58], Loss: 1.0749\n",
      "Epoch [5/5], Step [6/58], Loss: 0.9716\n",
      "Epoch [5/5], Step [7/58], Loss: 0.9091\n",
      "Epoch [5/5], Step [8/58], Loss: 1.0066\n",
      "Epoch [5/5], Step [9/58], Loss: 0.9038\n",
      "Epoch [5/5], Step [10/58], Loss: 1.1691\n",
      "Epoch [5/5], Step [11/58], Loss: 1.1637\n",
      "Epoch [5/5], Step [12/58], Loss: 0.9928\n",
      "Epoch [5/5], Step [13/58], Loss: 1.1700\n",
      "Epoch [5/5], Step [14/58], Loss: 1.0385\n",
      "Epoch [5/5], Step [15/58], Loss: 0.9111\n",
      "Epoch [5/5], Step [16/58], Loss: 1.1785\n",
      "Epoch [5/5], Step [17/58], Loss: 1.1517\n",
      "Epoch [5/5], Step [18/58], Loss: 1.0605\n",
      "Epoch [5/5], Step [19/58], Loss: 1.3413\n",
      "Epoch [5/5], Step [20/58], Loss: 1.0090\n",
      "Epoch [5/5], Step [21/58], Loss: 0.9936\n",
      "Epoch [5/5], Step [22/58], Loss: 1.5329\n",
      "Epoch [5/5], Step [23/58], Loss: 1.2410\n",
      "Epoch [5/5], Step [24/58], Loss: 1.2412\n",
      "Epoch [5/5], Step [25/58], Loss: 1.1811\n",
      "Epoch [5/5], Step [26/58], Loss: 0.9587\n",
      "Epoch [5/5], Step [27/58], Loss: 1.1551\n",
      "Epoch [5/5], Step [28/58], Loss: 1.0990\n",
      "Epoch [5/5], Step [29/58], Loss: 1.0833\n",
      "Epoch [5/5], Step [30/58], Loss: 1.1970\n",
      "Epoch [5/5], Step [31/58], Loss: 1.1594\n",
      "Epoch [5/5], Step [32/58], Loss: 1.1742\n",
      "Epoch [5/5], Step [33/58], Loss: 1.2723\n",
      "Epoch [5/5], Step [34/58], Loss: 1.0672\n",
      "Epoch [5/5], Step [35/58], Loss: 1.4430\n",
      "Epoch [5/5], Step [36/58], Loss: 1.0377\n",
      "Epoch [5/5], Step [37/58], Loss: 1.3428\n",
      "Epoch [5/5], Step [38/58], Loss: 1.0750\n",
      "Epoch [5/5], Step [39/58], Loss: 1.3312\n",
      "Epoch [5/5], Step [40/58], Loss: 1.1736\n",
      "Epoch [5/5], Step [41/58], Loss: 1.0755\n",
      "Epoch [5/5], Step [42/58], Loss: 1.0981\n",
      "Epoch [5/5], Step [43/58], Loss: 0.9379\n",
      "Epoch [5/5], Step [44/58], Loss: 1.2126\n",
      "Epoch [5/5], Step [45/58], Loss: 1.1338\n",
      "Epoch [5/5], Step [46/58], Loss: 1.1235\n",
      "Epoch [5/5], Step [47/58], Loss: 1.4332\n",
      "Epoch [5/5], Step [48/58], Loss: 1.1102\n",
      "Epoch [5/5], Step [49/58], Loss: 1.2247\n",
      "Epoch [5/5], Step [50/58], Loss: 1.9505\n",
      "Epoch [5/5], Step [51/58], Loss: 1.6007\n",
      "Epoch [5/5], Step [52/58], Loss: 1.3286\n",
      "Epoch [5/5], Step [53/58], Loss: 1.1223\n",
      "Epoch [5/5], Step [54/58], Loss: 1.1768\n",
      "Epoch [5/5], Step [55/58], Loss: 1.8337\n",
      "Epoch [5/5], Step [56/58], Loss: 1.1836\n",
      "Epoch [5/5], Step [57/58], Loss: 1.4334\n",
      "Epoch [5/5], Step [58/58], Loss: 1.5213\n"
     ]
    }
   ],
   "source": [
    "#hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "#getting datasets\n",
    "train_dataset, val_dataset, test_dataset = get_pet_datasets(img_width=128, img_height=128,root_path='./data' )\n",
    "\n",
    "#selecting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ConvNet(num_classes).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "#our dataloader for training dataset\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                .format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f8d3a61-d867-411a-bd17-06666e285bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves model to a dictionary on disk\n",
    "def save_model():\n",
    "    torch.save(model.state_dict(), 'model.ckpt')\n",
    "\n",
    "save_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64b8745a-86ce-4d04-b7c3-55fd010b105d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 40.08 %\n"
     ]
    }
   ],
   "source": [
    "def test_model():\n",
    "    model.eval()\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "    # Disable gradient calculation for efficiency\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "    \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print('Test Accuracy of the model on the 10000 test images: {:.2f} %'.format(100 * correct / total))\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5220f5ab-9d07-4a61-9586-fa1f24893bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
