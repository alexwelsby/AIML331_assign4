{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3ba0cb8-793b-4e4b-8545-39a5067909b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.data as data\n",
    "import torch.optim as optim\n",
    "import math\n",
    "\n",
    "## Torchvision\n",
    "import torchvision\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torchvision import transforms,datasets\n",
    "from dataset_wrapper import get_pet_datasets\n",
    "## Imports for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42224fef-fc43-45cf-94c3-4f0cba2cb806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU name: NVIDIA GeForce RTX 3070 Laptop GPU\n",
      "torch.Size([4, 3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 4\n",
    "img_h = 128;\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset, val_dataset, test_dataset = get_pet_datasets(img_width=img_h, img_height=img_h,root_path='./data' )\n",
    "\n",
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(dataset=val_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)\n",
    "\n",
    "\n",
    "#selecting device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#printing because my main kernel wants to be stuck on CPU-only pytorch fsr\n",
    "print(\"GPU name:\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"None\")\n",
    "\n",
    "# When iteration starts, queue and thread start to load data from files.\n",
    "data_iter = iter(train_loader)\n",
    "# Mini-batch images and labels.\n",
    "images_onebatch, labels = next(data_iter)\n",
    "print(images_onebatch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09fc1d56-5d9c-4866-ab76-7278817800cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VisionTransformer(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_channels, num_heads, num_layers, num_classes, para_embedding, patch_size, num_patches, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of the input feature vectors to the Transformer\n",
    "            hidden_dim - Dimensionality of the hidden layer in the feed-forward networks\n",
    "                         within the Transformer\n",
    "            num_channels - Number of channels of the input (3 for RGB)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            num_layers - Number of layers to use in the Transformer\n",
    "            num_classes - Number of classes to predict\n",
    "            patch_size - Number of pixels that the patches have per dimension\n",
    "            num_patches - Maximum number of patches an image can have\n",
    "            dropout - Amount of dropout to apply in the feed-forward network and\n",
    "                      on the input encoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "        self.para_embedding = para_embedding  # save it\n",
    "\n",
    "        # Layers/Networks\n",
    "        self.input_layer = nn.Linear(num_channels*(patch_size**2), embed_dim)\n",
    "        self.transformer = nn.Sequential(*[AttentionBlock(embed_dim, hidden_dim, num_heads, dropout=dropout) for _ in range(num_layers)])\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.LayerNorm(embed_dim),\n",
    "            nn.Linear(embed_dim, num_classes)\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Parameters/Embeddings\n",
    "        self.cls_token = nn.Parameter(torch.randn(1,1,embed_dim))\n",
    "        if self.para_embedding == \"Fixed\":\n",
    "            self.register_buffer('pos_embedding', get_sinusoidal_positional_embedding(1 + num_patches, embed_dim))#fixed position embedding \n",
    "        elif self.para_embedding == \"Learnable\":\n",
    "            self.pos_embedding = nn.Parameter(torch.randn(1,1+num_patches,embed_dim)) # learnable positional embedding\n",
    "        else:\n",
    "            pass\n",
    "            #no pos embedding lol\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Preprocess input\n",
    "        x = img_to_patch(x, self.patch_size)\n",
    "        B, T, C, Ph, Pw = x.shape\n",
    "        x = x.flatten(2,4)  # # Flatten channel and patch spatial dimensions(2-4) (C, patch_H, patch_W) -> (C * patch_H * patch_W)\n",
    "        B, T, _ = x.shape\n",
    "        x = self.input_layer(x)\n",
    "\n",
    "        # Add CLS token and positional encoding, summary token, the classification model will learn this token's attention on the entire sequence\n",
    "        cls_token = self.cls_token.repeat(B, 1, 1) # A learnable parameter\n",
    "        x = torch.cat([cls_token, x], dim=1)\n",
    "        if self.para_embedding is not None:\n",
    "            x = x + self.pos_embedding[:,:T+1] # CLS Token need positinal embedding\n",
    "\n",
    "        # Apply Transforrmer\n",
    "        x = self.dropout(x)\n",
    "        x = x.transpose(0, 1)\n",
    "        x = self.transformer(x)\n",
    "\n",
    "        # Perform classification prediction\n",
    "        cls = x[0]\n",
    "        out = self.mlp_head(cls)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "85c26e30-982e-49dc-9b72-5e47e61b090b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_dim, hidden_dim, num_heads, dropout=0.0):\n",
    "        \"\"\"\n",
    "        Inputs:\n",
    "            embed_dim - Dimensionality of input and attention feature vectors\n",
    "            hidden_dim - Dimensionality of hidden layer in feed-forward network\n",
    "                         (usually 2-4x larger than embed_dim)\n",
    "            num_heads - Number of heads to use in the Multi-Head Attention block\n",
    "            dropout - Amount of dropout to apply in the feed-forward network\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self.layer_norm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.attn = nn.MultiheadAttention(embed_dim, num_heads,\n",
    "                                          dropout=dropout)\n",
    "        self.layer_norm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.linear = nn.Sequential(\n",
    "            nn.Linear(embed_dim, hidden_dim), #expansion to four folder\n",
    "            nn.GELU(), # Gaussian Error Linear Units (GELUs)\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, embed_dim), #reduce the dimensionality back\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        inp_x = self.layer_norm_1(x)\n",
    "        x = x + self.attn(inp_x, inp_x, inp_x)[0]\n",
    "        x = x + self.linear(self.layer_norm_2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "617a3483-8a5d-4720-b1b3-2afff16027b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_sinusoidal_positional_embedding(n_positions, dim):\n",
    "    position = torch.arange(n_positions).unsqueeze(1)        # (n_positions, 1)\n",
    "    div_term = torch.exp(torch.arange(0, dim, 2) * (-np.log(10000.0) / dim))  # (dim/2,) #Avoids overflow/underflow in exp(log) way\n",
    "    \n",
    "    pe = torch.zeros(n_positions, dim)\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    \n",
    "    return pe.unsqueeze(0)  # shape: (1, n_positions, dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa15165f-28f9-4a7e-8782-a9c08c7fff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def img_to_patch(x, patch_size):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        x - torch.Tensor representing the image of shape [B, C, H, W]\n",
    "        patch_size - Number of pixels per dimension of the patches (integer)\n",
    "        flatten_channels - If True, the patches will be returned in a flattened format\n",
    "                           as a feature vector instead of a image grid.\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    x = x.reshape(B, C, H//patch_size, patch_size, W//patch_size, patch_size)#[B, C, H', p_H, W',p_W]\n",
    "    x = x.permute(0, 2, 4, 1, 3, 5) # [B, H', W', C, p_H, p_W]\n",
    "    x = x.flatten(1,2)              # [B, H'*W', C, p_H, p_W]\n",
    "    return x\n",
    "\n",
    "img_patches = img_to_patch(images_onebatch, patch_size=4)# CIFAR10 images are 32x32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3995a27-c888-4b28-a94f-35a4afe0f747",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_text_to_file(file_path, text_to_append):\n",
    "    try:\n",
    "        with open(file_path, 'a') as file:\n",
    "            file.write(text_to_append + '\\n')\n",
    "        print(f\"Text appended to {file_path} successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "af237db1-1c8f-47fa-a9d4-3d017e318260",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "def extract_models_from_file(filepath):\n",
    "    models_ive_trained = []\n",
    "\n",
    "     #creates file if it doesn't exist\n",
    "    if not os.path.exists(filepath):\n",
    "        with open(filepath, \"w\") as file:\n",
    "            pass  \n",
    "\n",
    "    # Read file\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Define a regex pattern to capture required parameters\n",
    "    pattern = re.compile(\n",
    "        r\"pos_embedding: (\\w+); num_heads: (\\d+); num_layers:(\\d+); patch_size: (\\d+)\",\n",
    "        re.IGNORECASE\n",
    "    )\n",
    "\n",
    "    # Process every line to match pattern\n",
    "    for line in lines:\n",
    "        match = pattern.search(line)\n",
    "        if match:\n",
    "            pos_embedding, num_heads, num_layers, patch_size = match.groups()\n",
    "            model_string = f\"{pos_embedding}{num_heads}{num_layers}{patch_size}\"\n",
    "            models_ive_trained.append(model_string)\n",
    "\n",
    "    return models_ive_trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ae7747b1-da0b-44ae-be34-39b439f3be3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training loss\n",
    "def generate_charts(num_layers, num_heads, pos_embedding, patch_size, train_losses, val_accuracies):\n",
    "    #print(f\"train_losses in gen_charts: {len(train_losses)}\")\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(train_losses, label=f'Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss Curve \\n({num_layers} layers, {num_heads} heads, pos_embedding={pos_embedding}, patch_size={patch_size})')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot validation accuracy\n",
    "    #print(f\"validation accuracy in gen_charts: {len(val_accuracies)}\")\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(val_accuracies, label=f'Validation Accuracy', color='green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title(f'Validation Accuracy Curve \\n({num_layers} layers, {num_heads} heads, pos_embedding={pos_embedding}, patch_size={patch_size})')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'part2_layers{num_layers}_numHeads{num_heads}_embedding{pos_embedding}_patchSize{patch_size}.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dac96871-1dbc-4f09-b959-9b98fa5ab3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, descript, batch_size, device):\n",
    "    # --- Test the model ---\n",
    "    model.eval()  # evaluation mode\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)# choose the class that have the highest score\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "        print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n",
    "        #Remember guessing randomly among 10 classes would be about 25% accuracy\n",
    "    \n",
    "    new_acc_descript = f'       Test Accuracy of the model on the {total} test images: {(100 * correct / total)}'\n",
    "    append_text_to_file('Part2AllAccuracies.txt', descript + \"\\n\" + new_acc_descript)\n",
    "    print(descript + \"\\n\" + new_acc_descript)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b32028-b396-4a85-b71a-d247a2962626",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_embedding: Fixed; num_heads: 4; num_layers:3; patch_size: 4\n",
      "Epoch [1/25], Step [1430/1430], Loss: 1.2959\n",
      "Epoch [2/25], Step [1430/1430], Loss: 1.3293\n",
      "Epoch [3/25], Step [1430/1430], Loss: 1.0323\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32 #I used an old version of the assignment brief that stated embed_dim=32. As a result, I can also only use 4 heads\n",
    "hidden_dim = 256\n",
    "num_channels = 3\n",
    "num_head = 4\n",
    "num_layers = [3, 4, 5, 6] \n",
    "num_classes = 4 #we have 4 classes\n",
    "patch_size = [4, 8, 16]\n",
    "para_embedding = [ \"Fixed\", \"Learnable\", None ]\n",
    "\n",
    "filepath = \"Part2AllAccuracies.txt\"\n",
    "models_ive_trained = extract_models_from_file(filepath)\n",
    "\n",
    "learning_rate = 0.001\n",
    "num_epochs = 25  #often trained for 300–500 epochs \n",
    "\n",
    "\n",
    "for a in range(len(para_embedding)):\n",
    "    embedding_bool = para_embedding[a]\n",
    "    for w in range(len(num_layers)):\n",
    "        num_layer = num_layers[w]\n",
    "        for e in range(len(patch_size)):\n",
    "            cur_patch_size = patch_size[e]\n",
    "            #checking if i've already trained this kind of model (and thus it can be skipped)\n",
    "            #i use this in case a meteor hit my computer mid-training\n",
    "            this_model = f\"{embedding_bool}{num_head}{num_layer}{cur_patch_size}\"\n",
    "            if this_model in models_ive_trained:\n",
    "                print(\"We've already trained this model, it's getting skipped.\")\n",
    "            else:\n",
    "                num_patches = (128//cur_patch_size)*(128//cur_patch_size)  #img is 128x128\n",
    "\n",
    "                #LETS GOOO\n",
    "                model = VisionTransformer(embed_dim, hidden_dim, num_channels, num_heads=num_head, num_layers=num_layer, num_classes=num_classes, patch_size=cur_patch_size,para_embedding=embedding_bool, num_patches=num_patches).to(device)\n",
    "\n",
    "                # Loss and optimizer\n",
    "                criterion = nn.CrossEntropyLoss() # Feel free to use other loss\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # Feel free to try other optimizer, e,g,SGD (Stochastic Gradient Descent), Adagrad\n",
    "\n",
    "                train_losses = []\n",
    "                val_accuracies = []\n",
    "\n",
    "                 # --- Train the model ---\n",
    "                #making a description of parameters that will print to both the console and (at the end) to a .txt file\n",
    "                descript = f\"pos_embedding: {embedding_bool}; num_heads: {num_head}; num_layers:{num_layer}; patch_size: {cur_patch_size}\";\n",
    "                print(descript)\n",
    "                \n",
    "                total_step = len(train_loader)\n",
    "                for epoch in range(num_epochs):\n",
    "                    #entering training mode (important as we entered eval mode during val)\n",
    "                    model.train()\n",
    "                    epoch_loss = 0\n",
    "                    for i, (images, labels) in enumerate(train_loader):\n",
    "                        images = images.to(device)\n",
    "                        labels = labels.to(device)\n",
    "                        \n",
    "                        # Forward pass\n",
    "                        outputs = model(images)\n",
    "                        loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        # Backward and optimize\n",
    "                        optimizer.zero_grad()\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                        epoch_loss += loss.item()   \n",
    "                        if (i+1) % 1430 == 0:\n",
    "                            print(f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{total_step}], Loss: {loss.item():.4f}')\n",
    "\n",
    "                    avg_loss = epoch_loss / total_step\n",
    "                    train_losses.append(avg_loss)\n",
    "    \n",
    "                    #now we do validation. exit training mode\n",
    "                    model.eval()\n",
    "                    correct = 0                       \n",
    "                    total = 0\n",
    "                    with torch.no_grad():\n",
    "                        for images, labels in val_loader:\n",
    "                            images = images.to(device)\n",
    "                            labels = labels.to(device)\n",
    "                            outputs = model(images)\n",
    "                            _, predicted = torch.max(outputs.data, 1)\n",
    "                            total += labels.size(0)\n",
    "                            correct += (predicted == labels).sum().item()\n",
    "                    accuracy = 100 * correct / total\n",
    "                    val_accuracies.append(accuracy)\n",
    "                    #print(f\"length of val_accuracies in run_model: {len(val_accuracies)}\")\n",
    "                generate_charts(num_layers=num_layer, num_heads=num_head,pos_embedding=embedding_bool, patch_size=cur_patch_size, train_losses=train_losses, val_accuracies=val_accuracies)                    \n",
    "                test_model(model, descript, batch_size, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993030be-aa3c-4770-98f7-441a5cd65cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2729944c-82ad-42f6-92b2-1fe0c6f31972",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (awesome kernel)",
   "language": "python",
   "name": "awesome_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
